{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a56d995-4020-4a73-bf4d-c70bf19e6a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages',\n",
       " '/scratch/users/njain26',\n",
       " '',\n",
       " '/share/software/user/open/py-jupyterlab/4.0.8_py39/lib/python3.9/site-packages',\n",
       " '/share/software/user/open/python/3.9.0/lib/python39.zip',\n",
       " '/share/software/user/open/python/3.9.0/lib/python3.9',\n",
       " '/share/software/user/open/python/3.9.0/lib/python3.9/lib-dynload',\n",
       " '/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import typing_extensions\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d446f-a446-4a3c-a96a-16a61a57307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds_test = load_dataset(\"osunlp/TravelPlanner\", \"test\")['test']\n",
    "ds_train = load_dataset(\"osunlp/TravelPlanner\", \"train\")['train']\n",
    "ds_val = load_dataset(\"osunlp/TravelPlanner\", \"validation\")['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f165b2-13af-478c-88e2-52504af2ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['org', 'dest', 'days', 'visiting_city_number', 'date', 'people_number',\n",
      "       'local_constraint', 'budget', 'query', 'level', 'annotated_plan',\n",
      "       'reference_information'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = []\n",
    "for item in ds_train:\n",
    "    data.append({\n",
    "        'org': item.get('org'),\n",
    "        'dest': item.get('dest'),\n",
    "        'days': item.get('days'),\n",
    "        'visiting_city_number': item.get('visiting_city_number'),\n",
    "        'date': item.get('date'),\n",
    "        'people_number': item.get('people_number'),\n",
    "        'local_constraint': item.get('local_constraint'),\n",
    "        'budget': item.get('budget'),\n",
    "        'query': item.get('query'),\n",
    "        'level': item.get('level'),\n",
    "        'annotated_plan': item.get('annotated_plan'),\n",
    "        'reference_information': item.get('reference_information')\n",
    "    })\n",
    "df_train = pd.DataFrame(data)\n",
    "\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcd971-cf40-4b2b-b574-0f0b0d6c2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_prompt(row):\n",
    "    \"\"\"\n",
    "    Creates a system prompt from a row in the travel dataset.\n",
    "    \n",
    "    Args:\n",
    "        row: A pandas DataFrame row containing travel information\n",
    "        \n",
    "    Returns:\n",
    "        A string with the formatted system prompt\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a professional travel agent. Create a detailed travel plan based on the following information:\n",
    "\n",
    "Origin: {row['org']}\n",
    "Destination: {row['dest']}\n",
    "Duration: {row['days']} days\n",
    "Travel Date: {row['date']}\n",
    "Number of People: {row.get('people_number', 'Not specified')}\n",
    "Budget: {row.get('budget', 'Not specified')}\n",
    "\n",
    "Instructions:\n",
    "1. Enclose your thinking process in <think></think> tags\n",
    "2. Enclose your final answer in <answer></answer> tags\n",
    "3. When checking for flights or hotels, enclose those API calls in <tool></tool> tags\n",
    "\n",
    "For example:\n",
    "<think>\n",
    "I'll need to find suitable flights from {row['org']} to {row['dest']}, then look for accommodations within the budget.\n",
    "</think>\n",
    "\n",
    "<tool>\n",
    "SearchFlights(origin=\"{row['org']}\", destination=\"{row['dest']}\", date=\"{row['date']}\", passengers={row.get('people_number', 1)})\n",
    "</tool>\n",
    "\n",
    "<answer>\n",
    "Here is your personalized travel itinerary...\n",
    "</answer>\n",
    "\n",
    "Please provide a comprehensive travel plan that includes transportation and accomidations.\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "df_train['prompt'] = df_train.apply(create_system_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2e44f1-8677-45a5-a072-23119dc5ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reward_function(prompts, completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates model responses based on adherence to tag syntax and correct tool usage.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of input prompts\n",
    "        completions: List of model completions/responses\n",
    "        **kwargs: Additional keyword arguments\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Reward scores between 0.0 and 1.0 for each completion\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    for i, (prompt, completion) in enumerate(zip(prompts, completions)):\n",
    "        # Handle different completion formats that TRL might pass\n",
    "        if isinstance(completion, list) and len(completion) > 0 and isinstance(completion[0], dict):\n",
    "            completion_content = completion[0][\"content\"]\n",
    "        else:\n",
    "            completion_content = completion\n",
    "            \n",
    "        # Extract travel details from the prompt\n",
    "        travel_details = {}\n",
    "        \n",
    "        # Extract origin\n",
    "        origin_match = re.search(r'from\\s+(\\w+)', prompt, re.IGNORECASE)\n",
    "        if origin_match:\n",
    "            travel_details['org'] = origin_match.group(1)\n",
    "            \n",
    "        # Extract destination\n",
    "        dest_match = re.search(r'to\\s+(\\w+)', prompt, re.IGNORECASE)\n",
    "        if dest_match:\n",
    "            travel_details['dest'] = dest_match.group(1)\n",
    "            \n",
    "        # Extract people number\n",
    "        people_match = re.search(r'with\\s+(\\d+)\\s+people', prompt, re.IGNORECASE)\n",
    "        if people_match:\n",
    "            travel_details['people_number'] = people_match.group(1)\n",
    "            \n",
    "        # Extract dates\n",
    "        date_match = re.search(r'starting\\s+on\\s+(\\S+)', prompt, re.IGNORECASE)\n",
    "        if date_match:\n",
    "            travel_details['date'] = date_match.group(1)\n",
    "            \n",
    "        # Extract days\n",
    "        days_match = re.search(r'for\\s+(\\d+)\\s+days', prompt, re.IGNORECASE)\n",
    "        if days_match:\n",
    "            travel_details['days'] = days_match.group(1)\n",
    "        \n",
    "        # Base scoring\n",
    "        base_score = 0.0\n",
    "        max_score = 1.0\n",
    "        \n",
    "        # Check for basic tag structure\n",
    "        has_think_tags = bool(re.search(r'<think>.*?</think>', completion_content, re.DOTALL))\n",
    "        has_answer_tags = bool(re.search(r'<answer>.*?</answer>', completion_content, re.DOTALL))\n",
    "        \n",
    "        # Check for tool formats\n",
    "        flights_pattern = r'<tool>\\s*SearchFlights\\(.*?\\)\\s*</tool>'\n",
    "        hotel_pattern = r'<tool>\\s*SearchHotel\\(.*?\\)\\s*</tool>'\n",
    "        \n",
    "        has_flights_tool = bool(re.search(flights_pattern, completion_content, re.DOTALL))\n",
    "        has_hotel_tool = bool(re.search(hotel_pattern, completion_content, re.DOTALL))\n",
    "        \n",
    "        # Initialize scores for different components\n",
    "        think_score = 0.2 if has_think_tags else 0.0\n",
    "        answer_score = 0.3 if has_answer_tags else 0.0\n",
    "        tool_score = 0.0\n",
    "        \n",
    "        # Evaluate tool usage for flights\n",
    "        if has_flights_tool:\n",
    "            # Base score for having the correct flights tool format\n",
    "            flight_score = 0.25  \n",
    "            \n",
    "            # Extract all SearchFlights calls\n",
    "            search_flights_calls = re.findall(r'<tool>\\s*SearchFlights\\((.*?)\\)\\s*</tool>', completion_content, re.DOTALL)\n",
    "            \n",
    "            if search_flights_calls:\n",
    "                # Check for parameter correctness\n",
    "                param_scores = []\n",
    "                \n",
    "                for call in search_flights_calls:\n",
    "                    param_score = 0.0\n",
    "                    \n",
    "                    # Check origin parameter\n",
    "                    if travel_details.get('org') and re.search(fr'origin\\s*=\\s*[\"\\']?{travel_details[\"org\"]}[\"\\']?', call, re.IGNORECASE):\n",
    "                        param_score += 0.2\n",
    "                    \n",
    "                    # Check destination parameter\n",
    "                    if travel_details.get('dest') and re.search(fr'destination\\s*=\\s*[\"\\']?{travel_details[\"dest\"]}[\"\\']?', call, re.IGNORECASE):\n",
    "                        param_score += 0.2\n",
    "                    \n",
    "                    # Check passengers parameter\n",
    "                    if travel_details.get('people_number') and re.search(fr'passengers\\s*=\\s*{travel_details[\"people_number\"]}', call):\n",
    "                        param_score += 0.1\n",
    "                    \n",
    "                    # Check date parameter (just check if it exists)\n",
    "                    if re.search(r'date\\s*=\\s*[\\'\\\"]', call):\n",
    "                        param_score += 0.1\n",
    "                    \n",
    "                    param_scores.append(param_score)\n",
    "                \n",
    "                # Adjust flight score based on parameter correctness (max additional 0.15)\n",
    "                if param_scores:\n",
    "                    avg_param_score = sum(param_scores) / len(param_scores)\n",
    "                    flight_score += min(0.15, avg_param_score)\n",
    "            \n",
    "            tool_score += flight_score\n",
    "        \n",
    "        if has_hotel_tool:\n",
    "\n",
    "            hotel_score = 0.25\n",
    "            search_hotel_calls = re.findall(r'<tool>\\s*SearchHotel\\((.*?)\\)\\s*</tool>', completion_content, re.DOTALL)\n",
    "            \n",
    "            if search_hotel_calls:\n",
    "                param_scores = []\n",
    "                \n",
    "                for call in search_hotel_calls:\n",
    "                    param_score = 0.0\n",
    "                    \n",
    "                    # Check location parameter (should match destination)\n",
    "                    if travel_details.get('dest') and re.search(fr'location\\s*=\\s*[\"\\']?{travel_details[\"dest\"]}[\"\\']?', call, re.IGNORECASE):\n",
    "                        param_score += 0.2\n",
    "                    \n",
    "                    # Check check_in parameter\n",
    "                    if re.search(r'check_in\\s*=\\s*[\\'\\\"]', call):\n",
    "                        param_score += 0.1\n",
    "                        \n",
    "                    # Check check_out parameter \n",
    "                    if re.search(r'check_out\\s*=\\s*[\\'\\\"]', call):\n",
    "                        param_score += 0.1\n",
    "                    \n",
    "                    # Check guests parameter\n",
    "                    if travel_details.get('people_number') and re.search(fr'guests\\s*=\\s*{travel_details[\"people_number\"]}', call):\n",
    "                        param_score += 0.2\n",
    "                    \n",
    "                    param_scores.append(param_score)\n",
    "                \n",
    "                if param_scores:\n",
    "                    avg_param_score = sum(param_scores) / len(param_scores)\n",
    "                    hotel_score += min(0.15, avg_param_score)\n",
    "            \n",
    "            tool_score += hotel_score\n",
    "        \n",
    "        final_score = base_score + think_score + answer_score + tool_score\n",
    "        \n",
    "        # Normalize to ensure maximum is 1.0\n",
    "        rewards.append(min(max_score, final_score))\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        rw = rewards[i]\n",
    "        print(f\"\\n\\nreward: {rw}\")\n",
    "        print(\"prompt:\")\n",
    "        print(prompts[i])\n",
    "        print(\"prompt\")    \n",
    "        print(\"completion:\")\n",
    "        print(completions[i])\n",
    "        print(\"completion\\n\\n\")\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c9c8aa-2194-4c6f-b2f3-a6d53229f0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4caaaf-8070-40e9-8ae9-75113c9724fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3430cee-3cfb-47f4-a289-8940e45ce7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setting GRPO training parameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-travel-agent-grpo-run2\",\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    bf16=torch.cuda.is_available(),  \n",
    "    fp16=not torch.cuda.is_available() and torch.cuda.is_available(),  \n",
    "    \n",
    "    max_completion_length=512,\n",
    "    num_generations=4,\n",
    "    max_prompt_length=256,\n",
    "\n",
    "    optim=\"adamw_torch\",\n",
    "    gradient_checkpointing=True,  \n",
    "    \n",
    "    report_to=\"wandb\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1,\n",
    "    evaluation_strategy=\"no\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7a593a8-65a3-4f32-886e-d6188526742f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnjain26\u001b[0m (\u001b[33mnjain26-stanford-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/scratch/users/njain26/wandb/run-20250313_003111-v15pf7e4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/njain26-stanford-university/huggingface/runs/v15pf7e4' target=\"_blank\">qwen-travel-agent-grpo-run2</a></strong> to <a href='https://wandb.ai/njain26-stanford-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/njain26-stanford-university/huggingface' target=\"_blank\">https://wandb.ai/njain26-stanford-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/njain26-stanford-university/huggingface/runs/v15pf7e4' target=\"_blank\">https://wandb.ai/njain26-stanford-university/huggingface/runs/v15pf7e4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GRPOTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     reward_funcs\u001b[38;5;241m=\u001b[39m[reward_function],\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2546\u001b[0m )\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2554\u001b[0m ):\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/transformers/trainer.py:3692\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[1;32m   3690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m-> 3692\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[1;32m   3694\u001b[0m     loss_mb \u001b[38;5;241m=\u001b[39m smp_forward_backward(model, inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps)\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/trl/extras/profiling.py:87\u001b[0m, in \u001b[0;36mprofiling_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m profiling_context(\u001b[38;5;28mself\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:692\u001b[0m, in \u001b[0;36mGRPOTrainer._prepare_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_iterations \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 692\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_and_score_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffered_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps] \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/trl/trainer/grpo_trainer.py:758\u001b[0m, in \u001b[0;36mGRPOTrainer._generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;66;03m# Regular generation path\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m unwrap_model_for_generation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator) \u001b[38;5;28;01mas\u001b[39;00m unwrapped_model:\n\u001b[0;32m--> 758\u001b[0m         prompt_completion_ids \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;66;03m# Compute prompt length and extract completion ids\u001b[39;00m\n\u001b[1;32m    763\u001b[0m     prompt_length \u001b[38;5;241m=\u001b[39m prompt_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/scratch/users/njain26/grpo-venv/lib/python3.9/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3256\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3257\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3258\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3259\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    reward_funcs=[reward_function],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7286c-0d46-4571-9385-601b78bf5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)\n",
    "trainer.push_to_hub(dataset_name=dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db889a28-28f4-4e24-bb4a-18ca8b0614c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\",\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_response(model, tokenizer, system_prompt):\n",
    "    \"\"\"Generate a response from the model using only a system prompt.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    response = full_output[len(prompt):].strip()\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bb20c-0f00-44cb-be3b-67bab9544943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /scratch/users/njain26/qwen-travel-agent-grpo-run2/checkpoint-8/\n"
     ]
    }
   ],
   "source": [
    "grpo_model, grpo_tokenizer = load_model_and_tokenizer(\"/scratch/users/njain26/qwen-travel-agent-grpo-run2/checkpoint-9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e2cb3e-ba2c-4225-87b5-3c42eaa9d395",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_model_and_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m baseline_model, baseline_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-1.5B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_model_and_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_model, baseline_tokenizer = load_model_and_tokenizer(\"Qwen/Qwen2.5-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "834d5a7d-577e-4137-8ca4-c9c17e489c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "large_model, large_tokenizer = load_model_and_tokenizer(\"Qwen/Qwen2.5-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc84d861-fcca-4a5a-9c30-01be1b1a7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for item in ds_test:\n",
    "    test_data.append({\n",
    "        'org': item.get('org'),\n",
    "        'dest': item.get('dest'),\n",
    "        'days': item.get('days'),\n",
    "        'visiting_city_number': item.get('visiting_city_number'),\n",
    "        'date': item.get('date'),\n",
    "        'people_number': item.get('people_number'),\n",
    "        'local_constraint': item.get('local_constraint'),\n",
    "        'budget': item.get('budget'),\n",
    "        'query': item.get('query'),\n",
    "        'level': item.get('level'),\n",
    "        'annotated_plan': item.get('annotated_plan'),\n",
    "        'reference_information': item.get('reference_information')\n",
    "    })\n",
    "df_test = pd.DataFrame(test_data)\n",
    "\n",
    "df_test['prompt'] = df_test.apply(create_system_prompt, axis=1)\n",
    "df_sample = df_test.sample(n=200, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be153eb-06a0-44f4-a198-32cf1dc9a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_response_dataframe(model, tokenizer, dataset):\n",
    "    results = []\n",
    "    \n",
    "    for example in tqdm(dataset, desc=\"Generating responses\"):\n",
    "        # Get only the system prompt from the dataset\n",
    "        system_prompt = example.get(\"prompt\", \"You are a helpful AI travel planning agent.\")\n",
    "        response = generate_response(model, tokenizer, system_prompt)\n",
    "        \n",
    "        # Result dictionary with all original fields plus the response\n",
    "        result = {**example, \"model_response\": response}\n",
    "        results.append(result)\n",
    "    \n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb8b0a7-c7c9-4570-baac-cdaf939b924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = Dataset.from_pandas(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b75d1e-d1cb-40d7-b538-3559bca7652f",
   "metadata": {},
   "source": [
    "output_data = Dataset.from_pandas(df_sample)\n",
    "output_test = create_response_dataframe(grpo_model, grpo_tokenizer, output_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b96dd792-f6f5-42a3-98b0-b372a07b60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [1:50:55<00:00, 33.28s/it]\n"
     ]
    }
   ],
   "source": [
    "output_test = create_response_dataframe(grpo_model, grpo_tokenizer, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2261e269-c0d8-4812-8aff-9f346b3cd3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [25:20<00:00,  7.60s/it]\n"
     ]
    }
   ],
   "source": [
    "baseline_test = create_response_dataframe(baseline_model, baseline_tokenizer, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9cd0651-bb24-4595-88e2-083b887d31a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [1:14:21<00:00, 22.31s/it]\n"
     ]
    }
   ],
   "source": [
    "large_test = create_response_dataframe(large_model,large_tokenizer, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce8a056-d624-48ab-b715-7867b10eee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test.to_csv(\"model_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a80cb79-976e-45bd-9d1e-ca87478373b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_test.to_csv(\"baseline_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8934940b-b898-4110-85b9-9220f5b6f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_test.to_csv(\"large_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f60fdad-8e94-43c3-831e-1c44aafec57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               org            dest  days visiting_city_number  \\\n",
      "0         Hartford           Texas     5                 None   \n",
      "1        Knoxville         Atlanta     3                 None   \n",
      "2          Chicago          Newark     3                 None   \n",
      "3           Tucson        Illinois     7                 None   \n",
      "4      Little Rock         Atlanta     3                 None   \n",
      "..             ...             ...   ...                  ...   \n",
      "195        Memphis           Tampa     3                 None   \n",
      "196       Key West  North Carolina     7                 None   \n",
      "197   Palm Springs           Texas     5                 None   \n",
      "198       San Jose      Washington     7                 None   \n",
      "199  San Francisco           Tampa     3                 None   \n",
      "\n",
      "                                                  date people_number  \\\n",
      "0    ['2022-03-04', '2022-03-05', '2022-03-06', '20...          None   \n",
      "1           ['2022-03-06', '2022-03-07', '2022-03-08']          None   \n",
      "2           ['2022-03-05', '2022-03-06', '2022-03-07']          None   \n",
      "3    ['2022-03-25', '2022-03-26', '2022-03-27', '20...          None   \n",
      "4           ['2022-03-12', '2022-03-13', '2022-03-14']          None   \n",
      "..                                                 ...           ...   \n",
      "195         ['2022-03-09', '2022-03-10', '2022-03-11']          None   \n",
      "196  ['2022-03-08', '2022-03-09', '2022-03-10', '20...          None   \n",
      "197  ['2022-03-25', '2022-03-26', '2022-03-27', '20...          None   \n",
      "198  ['2022-03-09', '2022-03-10', '2022-03-11', '20...          None   \n",
      "199         ['2022-03-24', '2022-03-25', '2022-03-26']          None   \n",
      "\n",
      "    local_constraint budget  \\\n",
      "0               None   None   \n",
      "1               None   None   \n",
      "2               None   None   \n",
      "3               None   None   \n",
      "4               None   None   \n",
      "..               ...    ...   \n",
      "195             None   None   \n",
      "196             None   None   \n",
      "197             None   None   \n",
      "198             None   None   \n",
      "199             None   None   \n",
      "\n",
      "                                                 query   level annotated_plan  \\\n",
      "0    Can you assist me in creating a 5-day travel p...  medium           None   \n",
      "1    Please create a travel plan for two people fro...    hard           None   \n",
      "2    Could you design a travel plan for two people ...    hard           None   \n",
      "3    I need a 7-day travel plan for 2 people, start...  medium           None   \n",
      "4    Could you prepare a 3-day travel itinerary, fr...  medium           None   \n",
      "..                                                 ...     ...            ...   \n",
      "195  Could you craft a 3-day travel plan for two, d...  medium           None   \n",
      "196  Could you devise a 7-day travel itinerary begi...    easy           None   \n",
      "197  Could you create a travel plan for me departin...    easy           None   \n",
      "198  We need a travel itinerary for two individuals...  medium           None   \n",
      "199  Could you help create a travel plan where I de...    easy           None   \n",
      "\n",
      "                                 reference_information  \\\n",
      "0    [{'Description': 'Attractions in Houston', 'Co...   \n",
      "1    [{'Description': 'Attractions in Atlanta', 'Co...   \n",
      "2    [{'Description': 'Attractions in Newark', 'Con...   \n",
      "3    [{'Description': 'Attractions in Chicago', 'Co...   \n",
      "4    [{'Description': 'Attractions in Atlanta', 'Co...   \n",
      "..                                                 ...   \n",
      "195  [{'Description': 'Attractions in Tampa', 'Cont...   \n",
      "196  [{'Description': 'Attractions in Wilmington', ...   \n",
      "197  [{'Description': 'Attractions in Dallas', 'Con...   \n",
      "198  [{'Description': 'Attractions in Wenatchee', '...   \n",
      "199  [{'Description': 'Attractions in Tampa', 'Cont...   \n",
      "\n",
      "                                                prompt  __index_level_0__  \\\n",
      "0    You are a professional travel agent. Create a ...                521   \n",
      "1    You are a professional travel agent. Create a ...                737   \n",
      "2    You are a professional travel agent. Create a ...                740   \n",
      "3    You are a professional travel agent. Create a ...                660   \n",
      "4    You are a professional travel agent. Create a ...                411   \n",
      "..                                                 ...                ...   \n",
      "195  You are a professional travel agent. Create a ...                408   \n",
      "196  You are a professional travel agent. Create a ...                332   \n",
      "197  You are a professional travel agent. Create a ...                208   \n",
      "198  You are a professional travel agent. Create a ...                613   \n",
      "199  You are a professional travel agent. Create a ...                 78   \n",
      "\n",
      "                                        model_response  \n",
      "0    irements:\\n\\nDay 1: March 4th - March 5th\\n\\n*...  \n",
      "1    PetFriendly Hotel\\nLunch - Pizza or Mexican fo...  \n",
      "2    eling from Chicago to Newark on March 5th-7th,...  \n",
      "3    Tucson to three different cities in Illinois f...  \n",
      "4    your group in Little Rock and Atlanta:\\n\\n### ...  \n",
      "..                                                 ...  \n",
      "195  eeds:\\n\\nDay 1: March 9th\\n\\n- Start the day b...  \n",
      "196  rch 8, Key West\\n- Arrive at the airport\\n- Ch...  \n",
      "197  trip from Palm Springs to Texas:\\n\\nDay 1: Pal...  \n",
      "198  anion:\\n\\nDay 1: San Jose - Washington D.C.\\n-...  \n",
      "199  ay trip from San Francisco to Tampa:\\n\\nDay 1:...  \n",
      "\n",
      "[200 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(output_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71abc9-8228-402a-a9a5-d169914f43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"nopeeking!\")\n",
    "\n",
    "def create_evaluation_prompt(row):\n",
    "    \"\"\"\n",
    "    Create a prompt for the evaluator model to assess the quality of tool calls.\n",
    "    \n",
    "    Arguments:\n",
    "        row: DataFrame row containing model_response and annotated_plan\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation prompt string\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator of AI travel planning assistants. Your job is to score how well a model generates appropriate tool calls for flight and hotel searches.\n",
    "\n",
    "TRAVEL CONTEXT:\n",
    "{row['annotated_plan']}\n",
    "\n",
    "MODEL RESPONSE:\n",
    "{row['model_response']}\n",
    "\n",
    "EVALUATION INSTRUCTIONS:\n",
    "1. Focus ONLY on evaluating the quality of the \"search_flights\" and \"search_hotels\" tool calls, making sure they are present and properly called.\n",
    "2. De-emphasize any other aspects of the response like itinerary details or other tool calls, but use the context to make sure the model is not hallucinating, and generating relevant information to the original prompt.\n",
    "3. Score the response on a scale of 1-5, where:\n",
    "   - 5: Perfect tool calls with correct parameters matching the travel context\n",
    "   - 4: Good tool calls with minor issues\n",
    "   - 3: Acceptable tool calls but with noticeable errors\n",
    "   - 2: Poor tool calls with major errors\n",
    "   - 1: Missing or completely incorrect tool calls\n",
    "\n",
    "Please provide:\n",
    "1. A numeric score (1-5)\n",
    "2. A brief explanation of your reasoning (2-3 sentences)\n",
    "3. End with \"FINAL_SCORE: X\" where X is your numeric score\n",
    "\n",
    "Your evaluation:\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def generate_evaluation(prompt, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Generate an evaluation using OpenAI API.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert evaluator of travel planning AI responses.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2, \n",
    "            max_tokens=512\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling OpenAI API: {e}\")\n",
    "        time.sleep(5)\n",
    "        return \"Error: Could not generate evaluation. FINAL_SCORE: 0\"\n",
    "\n",
    "def extract_score(evaluation_text):\n",
    "    \"\"\"Extract the numerical score from the evaluation text.\"\"\"\n",
    "    if \"FINAL_SCORE:\" in evaluation_text:\n",
    "        try:\n",
    "            score_text = evaluation_text.split(\"FINAL_SCORE:\")[-1].strip()\n",
    "            score = float(score_text.split()[0])\n",
    "            return score\n",
    "        except:\n",
    "            import re\n",
    "            scores = re.findall(r'\\b[1-5]\\.?0?\\b', evaluation_text)\n",
    "            if scores:\n",
    "                return float(scores[-1])\n",
    "    \n",
    "    return None\n",
    "\n",
    "def evaluate_model_responses(results_df, batch_size=10, save_path=\"evaluation_results.csv\", model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Evaluate model responses using OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame containing model_response and annotated_plan columns\n",
    "        batch_size: Number of examples to evaluate before saving intermediate results\n",
    "        save_path: Path to save the results\n",
    "        model: OpenAI model to use for evaluation (\"gpt-3.5-turbo\" or \"gpt-4o-mini\")\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with evaluation results\n",
    "    \"\"\"\n",
    "    eval_df = results_df.copy()\n",
    "    eval_df['evaluation'] = None\n",
    "    eval_df['score'] = None\n",
    "    \n",
    "    # Process in batches \n",
    "    for i in tqdm(range(0, len(eval_df), batch_size), desc=\"Evaluating responses\"):\n",
    "        batch = eval_df.iloc[i:i+batch_size]\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            if pd.notna(eval_df.loc[idx, 'evaluation']):\n",
    "                continue\n",
    "            \n",
    "            eval_prompt = create_evaluation_prompt(row)\n",
    "            evaluation = generate_evaluation(eval_prompt, model=model)\n",
    "            score = extract_score(evaluation)\n",
    "            \n",
    "            eval_df.loc[idx, 'evaluation'] = evaluation\n",
    "            eval_df.loc[idx, 'score'] = score\n",
    "       \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        eval_df.to_csv(save_path, index=False)\n",
    "        print(f\"Saved intermediate results after {i+len(batch)} evaluations\")\n",
    "    \n",
    "    print(\"\\nEvaluation Summary:\")\n",
    "    print(f\"Average Score: {eval_df['score'].mean():.2f}\")\n",
    "    print(f\"Score Distribution:\")\n",
    "    print(eval_df['score'].value_counts().sort_index())\n",
    "    \n",
    "    return eval_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3900bd-c20b-4f5d-b476-af1d9660c55b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ad4b9-65e6-4ca4-a980-e192b985dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation_results = evaluate_model_responses(output_test, batch_size=10,model=\"gpt-4o-mini\")\n",
    "evaluation_results.to_csv(\"final_evaluation_grpo_results.csv\", index=False)\n",
    "\n",
    "summary = {\n",
    "    \"average_score\": float(evaluation_results['score'].mean()),\n",
    "    \"median_score\": float(evaluation_results['score'].median()),\n",
    "    \"score_distribution\": evaluation_results['score'].value_counts().sort_index().to_dict(),\n",
    "    \"total_evaluated\": len(evaluation_results),\n",
    "    \"examples\": {\n",
    "    \"best\": evaluation_results.loc[evaluation_results['score'].idxmax()].to_dict(),\n",
    "    \"worst\": evaluation_results.loc[evaluation_results['score'].idxmin()].to_dict(),\n",
    "    \"average\": evaluation_results.loc[(evaluation_results['score'] - evaluation_results['score'].mean()).abs().idxmin()].to_dict()\n",
    "    }\n",
    "}\n",
    "    \n",
    "with open(\"evaluation_summary.json\", \"w\") as f: json.dump(summary, f, indent=2)\n",
    "    \n",
    "print(f\"Evaluation complete. Results saved to final_evaluation_results.csv and evaluation_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c147b44-936d-4937-9854-e01331af7cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:   5%|â–Œ         | 1/20 [00:24<07:46, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 10 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  10%|â–ˆ         | 2/20 [00:48<07:14, 24.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 20 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  15%|â–ˆâ–Œ        | 3/20 [01:09<06:24, 22.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 30 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  20%|â–ˆâ–ˆ        | 4/20 [01:31<05:57, 22.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 40 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:00<06:13, 24.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 50 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:23<05:41, 24.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 60 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:50<05:24, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 70 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [03:13<04:54, 24.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 80 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:36<04:24, 24.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 90 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [04:00<03:59, 24.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 100 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [04:23<03:34, 23.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 110 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [04:47<03:09, 23.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 120 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [05:13<02:51, 24.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 130 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [05:39<02:28, 24.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 140 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [06:03<02:02, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 150 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [06:24<01:34, 23.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 160 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [06:51<01:14, 24.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 170 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [07:24<00:54, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 180 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [07:45<00:25, 25.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 190 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:10<00:00, 24.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 200 evaluations\n",
      "\n",
      "Evaluation Summary:\n",
      "Average Score: 3.54\n",
      "Score Distribution:\n",
      "score\n",
      "1.0     13\n",
      "2.0      8\n",
      "3.0     48\n",
      "4.0    120\n",
      "5.0     11\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluation_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m evaluation_results_baseline\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_evaluation_baseline_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Generate summary report\u001b[39;00m\n\u001b[1;32m      7\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(evaluation_results_baseline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(evaluation_results_baseline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmedian()),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_distribution\u001b[39m\u001b[38;5;124m\"\u001b[39m: evaluation_results_baseline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msort_index()\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_evaluated\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(evaluation_results_baseline),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexamples\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest\u001b[39m\u001b[38;5;124m\"\u001b[39m: evaluation_results_baseline\u001b[38;5;241m.\u001b[39mloc[\u001b[43mevaluation_results\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmax()]\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworst\u001b[39m\u001b[38;5;124m\"\u001b[39m: evaluation_results_baseline\u001b[38;5;241m.\u001b[39mloc[evaluation_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39midxmin()]\u001b[38;5;241m.\u001b[39mto_dict(),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage\u001b[39m\u001b[38;5;124m\"\u001b[39m: evaluation_results_baseline\u001b[38;5;241m.\u001b[39mloc[(evaluation_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m evaluation_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean())\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39midxmin()]\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m     16\u001b[0m     }\n\u001b[1;32m     17\u001b[0m }\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_summary_baseline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f: json\u001b[38;5;241m.\u001b[39mdump(summary, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation complete. Results saved to final_evaluation_baseline_results.csv and evaluation_summary_baseline.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation_results' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation_results_baseline = evaluate_model_responses(baseline_test, batch_size=10,model=\"gpt-4o-mini\")\n",
    "evaluation_results_baseline.to_csv(\"final_evaluation_baseline_results.csv\", index=False)\n",
    "\n",
    "summary = {\n",
    "    \"average_score\": float(evaluation_results_baseline['score'].mean()),\n",
    "    \"median_score\": float(evaluation_results_baseline['score'].median()),\n",
    "    \"score_distribution\": evaluation_results_baseline['score'].value_counts().sort_index().to_dict(),\n",
    "    \"total_evaluated\": len(evaluation_results_baseline),\n",
    "    \"examples\": {\n",
    "    \"best\": evaluation_results_baseline.loc[evaluation_results['score'].idxmax()].to_dict(),\n",
    "    \"worst\": evaluation_results_baseline.loc[evaluation_results['score'].idxmin()].to_dict(),\n",
    "    \"average\": evaluation_results_baseline.loc[(evaluation_results['score'] - evaluation_results['score'].mean()).abs().idxmin()].to_dict()\n",
    "    }\n",
    "}\n",
    "    \n",
    "with open(\"evaluation_summary_baseline.json\", \"w\") as f: json.dump(summary, f, indent=2)\n",
    "    \n",
    "print(f\"Evaluation complete. Results saved to final_evaluation_baseline_results.csv and evaluation_summary_baseline.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b44da62-a30f-49b7-9f6e-e3321685ad23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to determine the device handle for GPU0000:C4:00.0: Unknown Error\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add19c4b-95cb-41ed-90d1-57b37fbb8518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:   5%|â–Œ         | 1/20 [00:26<08:22, 26.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 10 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  10%|â–ˆ         | 2/20 [00:49<07:15, 24.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 20 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  15%|â–ˆâ–Œ        | 3/20 [01:14<07:00, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 30 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  20%|â–ˆâ–ˆ        | 4/20 [01:39<06:35, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 40 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  25%|â–ˆâ–ˆâ–Œ       | 5/20 [02:03<06:07, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 50 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  30%|â–ˆâ–ˆâ–ˆ       | 6/20 [02:31<05:59, 25.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 60 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [02:54<05:22, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 70 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [03:23<05:13, 26.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 80 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [03:48<04:43, 25.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 90 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [04:13<04:15, 25.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 100 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [04:41<03:56, 26.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 110 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [05:07<03:29, 26.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 120 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [05:34<03:05, 26.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 130 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [06:05<02:47, 27.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 140 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [06:33<02:19, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 150 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [07:05<01:56, 29.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 160 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [07:31<01:24, 28.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 170 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [07:56<00:54, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 180 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [08:20<00:26, 26.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 190 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating responses: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [08:44<00:00, 26.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved intermediate results after 200 evaluations\n",
      "\n",
      "Evaluation Summary:\n",
      "Average Score: 3.89\n",
      "Score Distribution:\n",
      "score\n",
      "3.0     25\n",
      "4.0    172\n",
      "5.0      3\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Results saved to final_evaluation_baseline_results.csv and evaluation_summary_baseline.json\n"
     ]
    }
   ],
   "source": [
    "evaluation_results_large = evaluate_model_responses(large_test, batch_size=10,model=\"gpt-4o-mini\")\n",
    "evaluation_results_large.to_csv(\"final_evaluation_large_results.csv\", index=False)\n",
    "   \n",
    "summary = {\n",
    "    \"average_score\": float(evaluation_results_large['score'].mean()),\n",
    "    \"median_score\": float(evaluation_results_large['score'].median()),\n",
    "    \"score_distribution\": evaluation_results_large['score'].value_counts().sort_index().to_dict(),\n",
    "    \"total_evaluated\": len(evaluation_results_large),\n",
    "    \"examples\": {\n",
    "    \"best\": evaluation_results_large.loc[evaluation_results_large['score'].idxmax()].to_dict(),\n",
    "    \"worst\": evaluation_results_large.loc[evaluation_results_large['score'].idxmin()].to_dict(),\n",
    "    \"average\": evaluation_results_large.loc[(evaluation_results_large['score'] - evaluation_results_large['score'].mean()).abs().idxmin()].to_dict()\n",
    "    }\n",
    "}\n",
    "    \n",
    "with open(\"evaluation_summary_baseline.json\", \"w\") as f: json.dump(summary, f, indent=2)\n",
    "    \n",
    "print(f\"Evaluation complete. Results saved to final_evaluation_baseline_results.csv and evaluation_summary_baseline.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grpo-venv",
   "language": "python",
   "name": "grpo-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
